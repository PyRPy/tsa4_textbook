index   = lag_test_tbl$index,
value   = (pred_out * scale_history + center_history)^2
)
# Combine actual data with predictions
tbl_1 <- df_trn %>%
add_column(key = "actual")
tbl_2 <- df_tst %>%
add_column(key = "actual")
tbl_3 <- pred_tbl %>%
add_column(key = "predict")
# Create time_bind_rows() to solve dplyr issue
time_bind_rows <- function(data_1, data_2, index) {
index_expr <- enquo(index)
bind_rows(data_1, data_2) %>%
as_tbl_time(index = !! index_expr)
}
ret <- list(tbl_1, tbl_2, tbl_3) %>%
reduce(time_bind_rows, index = index) %>%
arrange(key, index) %>%
mutate(key = as_factor(key))
return(ret)
}
safe_lstm <- possibly(lstm_prediction, otherwise = NA)
safe_lstm(split, epochs, ...)
}
predict_keras_lstm(split, epochs = 10)
# With the predict_keras_lstm() function in hand that works on one split, we can now map to all samples using a mutate() and map() combo. The predictions will be stored in a “list” column called “predict”.
sample_predictions_lstm_tbl <- rolling_origin_resamples %>%
mutate(predict = map(splits, predict_keras_lstm, epochs = 100))
predict_keras_lstm <- function(split, epochs = 300, ...) {
lstm_prediction <- function(split, epochs, ...) {
# 5.1.2 Data Setup
df_trn <- training(split)
df_tst <- testing(split)
df <- bind_rows(
df_trn %>% add_column(key = "training"),
df_tst %>% add_column(key = "testing")
) %>%
as_tbl_time(index = index)
# 5.1.3 Preprocessing
rec_obj <- recipe(value ~ ., df) %>%
step_sqrt(value) %>%
step_center(value) %>%
step_scale(value) %>%
prep()
df_processed_tbl <- bake(rec_obj, df)
center_history <- rec_obj$steps[[2]]$means["value"]
scale_history  <- rec_obj$steps[[3]]$sds["value"]
# 5.1.4 LSTM Plan
lag_setting  <- 120 # = nrow(df_tst)
batch_size   <- 40
train_length <- 440
tsteps       <- 1
epochs       <- epochs
# 5.1.5 Train/Test Setup
lag_train_tbl <- df_processed_tbl %>%
mutate(value_lag = lag(value, n = lag_setting)) %>%
filter(!is.na(value_lag)) %>%
filter(key == "training") %>%
tail(train_length)
x_train_vec <- lag_train_tbl$value_lag
x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
y_train_vec <- lag_train_tbl$value
y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
lag_test_tbl <- df_processed_tbl %>%
mutate(
value_lag = lag(value, n = lag_setting)
) %>%
filter(!is.na(value_lag)) %>%
filter(key == "testing")
x_test_vec <- lag_test_tbl$value_lag
x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
y_test_vec <- lag_test_tbl$value
y_test_arr <- array(data = y_test_vec, dim = c(length(y_test_vec), 1))
# 5.1.6 LSTM Model
model <- keras_model_sequential()
model %>%
layer_lstm(units            = 50,
input_shape      = c(tsteps, 1),
batch_size       = batch_size,
return_sequences = TRUE,
stateful         = TRUE) %>%
layer_lstm(units            = 50,
return_sequences = FALSE,
stateful         = TRUE) %>%
layer_dense(units = 1)
model %>%
compile(loss = 'mae', optimizer = 'adam')
# 5.1.7 Fitting LSTM
for (i in 1:epochs) {
model %>% fit(x          = x_train_arr,
y          = y_train_arr,
batch_size = batch_size,
epochs     = 1,
verbose    = 0,
shuffle    = FALSE)
model %>% reset_states()
cat("Epoch: ", i)
}
# 5.1.8 Predict and Return Tidy Data
# Make Predictions
pred_out <- model %>%
predict(x_test_arr, batch_size = batch_size) %>%
.[,1]
# Retransform values
pred_tbl <- tibble(
index   = lag_test_tbl$index,
value   = (pred_out * scale_history + center_history)^2
)
# Combine actual data with predictions
tbl_1 <- df_trn %>%
add_column(key = "actual")
tbl_2 <- df_tst %>%
add_column(key = "actual")
tbl_3 <- pred_tbl %>%
add_column(key = "predict")
# Create time_bind_rows() to solve dplyr issue
time_bind_rows <- function(data_1, data_2, index) {
index_expr <- enquo(index)
bind_rows(data_1, data_2) %>%
as_tbl_time(index = !! index_expr)
}
ret <- list(tbl_1, tbl_2, tbl_3) %>%
reduce(time_bind_rows, index = index) %>%
arrange(key, index) %>%
mutate(key = as_factor(key))
return(ret)
}
safe_lstm <- possibly(lstm_prediction, otherwise = NA)
safe_lstm(split, epochs, ...)
}
predict_keras_lstm(split, epochs = 10)
# With the predict_keras_lstm() function in hand that works on one split, we can now map to all samples using a mutate() and map() combo. The predictions will be stored in a “list” column called “predict”.
sample_predictions_lstm_tbl <- rolling_origin_resamples %>%
mutate(predict = map(splits, predict_keras_lstm, epochs = 100))
sample_predictions_lstm_tbl
#
# We can assess the RMSE by mapping the calc_rmse() function to the “predict” column.
sample_rmse_tbl <- sample_predictions_lstm_tbl %>%
mutate(rmse = map_dbl(predict, calc_rmse)) %>%
select(id, rmse)
sample_rmse_tbl
# With the predict_keras_lstm() function in hand that works on one split, we can now map to all samples using a mutate() and map() combo. The predictions will be stored in a “list” column called “predict”.
sample_predictions_lstm_tbl <- rolling_origin_resamples %>%
mutate(predict = map(splits, predict_keras_lstm, epochs = 300))
sample_predictions_lstm_tbl
# We can assess the RMSE by mapping the calc_rmse() function to the “predict” column.
sample_rmse_tbl <- sample_predictions_lstm_tbl %>%
mutate(rmse = map_dbl(predict, calc_rmse)) %>%
select(id, rmse)
sample_rmse_tbl
sample_rmse_tbl %>%
ggplot(aes(rmse)) +
geom_histogram(aes(y = ..density..), fill = palette_light()[[1]], bins = 16) +
geom_density(fill = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
ggtitle("Histogram of RMSE")
sample_rmse_tbl %>%
summarize(
mean_rmse = mean(rmse),
sd_rmse   = sd(rmse)
)
plot_predictions <- function(sampling_tbl, predictions_col,
ncol = 3, alpha = 1, size = 2, base_size = 14,
title = "Backtested Predictions") {
predictions_col_expr <- enquo(predictions_col)
# Map plot_split() to sampling_tbl
sampling_tbl_with_plots <- sampling_tbl %>%
mutate(gg_plots = map2(!! predictions_col_expr, id,
.f        = plot_prediction,
alpha     = alpha,
size      = size,
base_size = base_size))
# Make plots with cowplot
plot_list <- sampling_tbl_with_plots$gg_plots
p_temp <- plot_list[[1]] + theme(legend.position = "bottom")
legend <- get_legend(p_temp)
p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)
p_title <- ggdraw() +
draw_label(title, size = 18, fontface = "bold", colour = palette_light()[[1]])
g <- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))
return(g)
}
sample_predictions_lstm_tbl %>%
plot_predictions(predictions_col = predict, alpha = 0.5, size = 1, base_size = 10,
title = "Keras Stateful LSTM: Backtested Predictions")
# We can predict the next 10 years by adjusting the prediction function to work with the full data set.
predict_keras_lstm_future <- function(data, epochs = 300, ...) {
lstm_prediction <- function(data, epochs, ...) {
# 5.1.2 Data Setup (MODIFIED)
df <- data
# 5.1.3 Preprocessing
rec_obj <- recipe(value ~ ., df) %>%
step_sqrt(value) %>%
step_center(value) %>%
step_scale(value) %>%
prep()
df_processed_tbl <- bake(rec_obj, df)
center_history <- rec_obj$steps[[2]]$means["value"]
scale_history  <- rec_obj$steps[[3]]$sds["value"]
# 5.1.4 LSTM Plan
lag_setting  <- 120 # = nrow(df_tst)
batch_size   <- 40
train_length <- 440
tsteps       <- 1
epochs       <- epochs
# 5.1.5 Train Setup (MODIFIED)
lag_train_tbl <- df_processed_tbl %>%
mutate(value_lag = lag(value, n = lag_setting)) %>%
filter(!is.na(value_lag)) %>%
tail(train_length)
x_train_vec <- lag_train_tbl$value_lag
x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
y_train_vec <- lag_train_tbl$value
y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
x_test_vec <- y_train_vec %>% tail(lag_setting)
x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
# 5.1.6 LSTM Model
model <- keras_model_sequential()
model %>%
layer_lstm(units            = 50,
input_shape      = c(tsteps, 1),
batch_size       = batch_size,
return_sequences = TRUE,
stateful         = TRUE) %>%
layer_lstm(units            = 50,
return_sequences = FALSE,
stateful         = TRUE) %>%
layer_dense(units = 1)
model %>%
compile(loss = 'mae', optimizer = 'adam')
# 5.1.7 Fitting LSTM
for (i in 1:epochs) {
model %>% fit(x          = x_train_arr,
y          = y_train_arr,
batch_size = batch_size,
epochs     = 1,
verbose    = 1,
shuffle    = FALSE)
model %>% reset_states()
cat("Epoch: ", i)
}
# 5.1.8 Predict and Return Tidy Data (MODIFIED)
# Make Predictions
pred_out <- model %>%
predict(x_test_arr, batch_size = batch_size) %>%
.[,1]
# Make future index using tk_make_future_timeseries()
idx <- data %>%
tk_index() %>%
tk_make_future_timeseries(n_future = lag_setting)
# Retransform values
pred_tbl <- tibble(
index   = idx,
value   = (pred_out * scale_history + center_history)^2
)
# Combine actual data with predictions
tbl_1 <- df %>%
add_column(key = "actual")
tbl_3 <- pred_tbl %>%
add_column(key = "predict")
# Create time_bind_rows() to solve dplyr issue
time_bind_rows <- function(data_1, data_2, index) {
index_expr <- enquo(index)
bind_rows(data_1, data_2) %>%
as_tbl_time(index = !! index_expr)
}
ret <- list(tbl_1, tbl_3) %>%
reduce(time_bind_rows, index = index) %>%
arrange(key, index) %>%
mutate(key = as_factor(key))
return(ret)
}
safe_lstm <- possibly(lstm_prediction, otherwise = NA)
safe_lstm(data, epochs, ...)
}
future_sun_spots_tbl <- predict_keras_lstm_future(sun_spots, epochs = 300)
future_sun_spots_tbl %>%
filter_time("1900" ~ "end") %>%
plot_prediction(id = NULL, alpha = 0.4, size = 1.5) +
theme(legend.position = "bottom") +
ggtitle("Sunspots: Ten Year Forecast", subtitle = "Forecast Horizon: 2013 - 2023")
knitr::opts_chunk$set(echo = TRUE)
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)
# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)
# Visualization
library(cowplot)
# Preprocessing
library(recipes)
# Sampling / Accuracy
library(rsample)
library(yardstick)
# Modeling
library(keras)
library(tfruns)
# If you have not previously run Keras in R, you will need to install Keras using the install_keras() function.
# Install Keras if you have not installed before
## install_keras()
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)
# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)
# Visualization
library(cowplot)
# Preprocessing
library(recipes)
# Sampling / Accuracy
library(rsample)
library(yardstick)
# Modeling
library(keras)
library(tfruns)
# If you have not previously run Keras in R, you will need to install Keras using the install_keras() function.
# Install Keras if you have not installed before
## install_keras()
sun_spots <- datasets::sunspot.month %>%
tk_tbl() %>%
mutate(index = as_date(index)) %>%
as_tbl_time(index = index)
sun_spots
p1 <- sun_spots %>%
ggplot(aes(index, value)) +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "From 1749 to 2013 (Full Data Set)")
p2 <- sun_spots %>%
filter_time("start" ~ "1800") %>%
ggplot(aes(index, value)) +
geom_line(color = palette_light()[[1]], alpha = 0.5) +
geom_point(color = palette_light()[[1]]) +
geom_smooth(method = "loess", span = 0.2, se = FALSE) +
theme_tq() +
labs(title = "1749 to 1759 (Zoomed In To Show Changes over the Year)",
caption = "datasets::sunspot.month")
p_title <- ggdraw() +
draw_label("Sunspots", size = 18, fontface = "bold", colour = palette_light()[[1]])
plot_grid(p_title, p1, p2, ncol = 1, rel_heights = c(0.1, 1, 1))
library('astsa')
source('grid.r')
setwd("~/GitHub/tsa4_textbook")
source('grid.r')
###############
# pdf(file="chickline.pdf",width=7.25,height=2.75)
par(mar=c(2,2.5,0,0)+.5, mgp=c(1.6,.3,0), tcl=-.2, las=1, cex.axis=.8)
head(chicken)
plot(chicken)
class(chicken)
summary(fit <- lm(chicken~time(chicken)))
plot(chicken, ylab="cents per pound",  type='n', ylim=c(58,122), yaxt='n')
grid(lty=1)
abline(fit)
lines(chicken, col=4, lwd=2)
axis(2, at = seq(60, 120, by = 10), las=2 )
time(chicken)
length(time(chicken))
dev.off()
par(mfrow = c(3,1), mar=c(2,2.5,1,0)+.5, mgp=c(1.6,.6,0))
plot(cmort, main="Cardiovascular Mortality", xlab="", ylab="",   type='n')
grid(lty=1); lines(cmort)
plot(tempr, main="Temperature", xlab="", ylab="",   type='n')
grid(lty=1); lines(tempr)
plot(part, main="Particulates", xlab="", ylab="",   type='n')
grid(lty=1)
lines(part)
dev.off()
###########
## Fig.2.3.Scatter plot matrix showing relations between mortality,temperature,
## and pollution.
pairs(cbind(Mortality=cmort, Temperature=tempr, Particulates=part))
plot(tempr)
plot(cmort)
plot(part)
# plot(tempr)
# plot(cmort)
# plot(part)
dev.off()
par(mfrow = c(2,1), mar=c(2,2,1,0)+.5, mgp=c(1.6,.6,0), cex.main=1.05)
fit <- lm(chicken~time(chicken), na.action=NULL) # regress chicken on time
plot(resid(fit), xlab="", main="detrended",   type='n')
grid(lty=1); lines(resid(fit))
plot(diff(chicken),  main="first difference",   type='n')
grid(lty=1)
lines(diff(chicken))
class(resid(fit))
dev.off()
par(mfrow = c(3,1), mar=c(2,2,.75,0)+.5, mgp=c(1.6,.6,0))
acf(chicken, 48, xlab="", main='', panel.first=grid(lty=1))
mtext("chicken", side=3, line=.1, cex=1, font=2)
acf(resid(fit), 48, xlab="", main='', panel.first=grid(lty=1))
mtext("detrended", side=3, line=.1, cex=1, font=2)
acf(diff(chicken), 48, xlab="", main='', panel.first=grid(lty=1))
mtext("first difference", side=3, line=.1, cex=1, font=2)
mtext("LAG", side=1, line=1.5, cex=.76)
dev.off()
par(mfrow = c(2,1), mar=c(1.5,2,1,1)+.5, mgp=c(1.6,.6,0))
plot(diff(globtemp), xlab="",  type='n' )
mtext("Year", side=1, line=1.5)
grid(lty=1); lines(diff(globtemp))
mean(diff(globtemp))  # drift
acf(diff(globtemp), 25, xlab="", main='', panel.first=grid(lty=1))
mtext("LAG", side=1, line=1)
dev.off()
############
## Example2.7 Paleoclimatic Glacial Varves
par(mfrow = c(2,1), mar=c(2,1.5,.75,0)+.5, mgp=c(1.6,.6,0), cex.main=1.05)
plot(varve, main="varve", ylab="", xlab="",  type='n')
grid(lty=1)
lines(varve)
plot(log(varve), main="log(varve)", ylab="",   type='n')
grid(lty=1)
lines(log(varve))
dev.off()
##############
## Example2.8 Scatter plot Matrices,SOI and Recruitment
lag1.plot(soi, 12)
dev.off()
## Fig. 2.9. Scatterplot matrix of the Recruitment series, Rt, on
## the vertical axis plotted against the SOI series, St−h,
lag2.plot(soi, rec, 8)
dev.off()
#####################
## Example2.9 Regression with Lagged Variables(cont)
par(mar=c(3,3,1,1), mgp=c(1.6,.6,0))
dummy <- ifelse(soi<0, 0, 1)
fish <- ts.intersect(rec, soiL6=lag(soi,-6), dL6=lag(dummy,-6), dframe=TRUE)
fit <- lm(rec~ soiL6*dL6, data=fish, na.action=NULL)
summary(fit)
attach(fish)
plot(soiL6, rec, , panel.first=grid(lty=1), col=gray(.4))
lines(lowess(soiL6, rec), col=4, lwd=2)
points(soiL6, fitted(fit), pch='+', col=2)
detach(fish) # be sure to detach it
dev.off()
###################
## Example2.10 Using Regression to Discover a Signal in Noise
par(mfrow=c(2,1), mar=c(1,2.2,0,0)+.5, mgp=c(1.5,.6,0))
set.seed(90210)  # so you can reproduce these results
x <- 2*cos(2*pi*1:500/50 + .6*pi) + rnorm(500,0,5)
z1 <- cos(2*pi*1:500/50)
z2 <- sin(2*pi*1:500/50)
fit <- lm(x~0+z1+z2) # zero to exclude the intercept,
plot.ts(x, xlab='', type='n')
grid()
lines(x)
plot.ts(x, ylab=expression(hat(x)), type='n')
lines(x, col=gray(.5))
grid()
lines(fitted(fit), col=2, lwd=2)
dev.off()
###################
## Example2.10 Using Regression to Discover a Signal in Noise
par(mfrow=c(2,1), mar=c(1,2.2,0,0)+.5, mgp=c(1.5,.6,0))
set.seed(90210)  # so you can reproduce these results
x <- 2*cos(2*pi*1:500/50 + .6*pi) + rnorm(500,0,5)
z1 <- cos(2*pi*1:500/50)
z2 <- sin(2*pi*1:500/50)
fit <- lm(x~0+z1+z2) # zero to exclude the intercept,
plot.ts(x, xlab='', type='n')
grid()
lines(x)
plot.ts(x, ylab=expression(hat(x)), type='n')
lines(x, col=gray(.5))
grid()
lines(fitted(fit), col=2, lwd=2)
dev.off()
set.seed(90210)  # so you can reproduce these results
x <- 2*cos(2*pi*1:500/50 + .6*pi) + rnorm(500,0,5)
z1 <- cos(2*pi*1:500/50)
z2 <- sin(2*pi*1:500/50)
fit <- lm(x~0+z1+z2) # zero to exclude the intercept,
summary(fit)
plot(fitted(fit))
dev.off()
######################
## Fig.2.12.MovingaveragesmootherofSOI.
## Theinsertshowstheshapeofthemovingaverage (“boxcar”)kernel
par(mar=c(2.5,2.5,.5,.5), mgp=c(1.6,.6,0))
######################
## Fig.2.12.MovingaveragesmootherofSOI.
## Theinsertshowstheshapeofthemovingaverage (“boxcar”)kernel
par(mar=c(2.5,2.5,.5,.5), mgp=c(1.6,.6,0))
w1 <- c(.5, rep(1,11), .5)/12
soif1 <- filter(soi, sides=2, filter=w1)
plot(soi, ylim=c(-1,1.15),  type='n')
grid(lty=1)
lines(soi, col=gray(.5))
lines(soif1, lwd=2, col=4)
par(fig = c(.65, 1, .65, 1),   new = TRUE)
nwgts = c(rep(0,20),w1,rep(0,20))
plot(nwgts, type="l", ylim = c(-.02,.1), xaxt='n', yaxt='n', ann=FALSE)
dev.off()
###############################################
## Example2.12 Kernel Smoothing
par(mar=c(2.5,2.5,.5,.5), mgp=c(1.6,.6,0))
plot(soi, ylim=c(-1,1.15),   type='n')
grid(lty=1)
lines(soi, col=gray(.5))
lines(ksmooth(time(soi), soi, "normal", bandwidth=1), lwd=2, col=4)
par(fig = c(.65, 1, .65, 1), new = TRUE)
gauss <- function(x) 1/sqrt(2*pi) * exp(-(x^2)/2)
x <- seq(from = -3, to = 3, by = 0.001)
plot(x, gauss(x), type = "l", ylim = c(-.02,.45), lty = 1,  xaxt='n', yaxt='n', ann=FALSE)
